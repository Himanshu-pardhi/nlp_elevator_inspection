{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file for all codes (labels) \n",
    "all_df = pd.DataFrame()\n",
    "path = 'C:\\\\Users\\\\user\\\\Desktop\\\\AI projects\\\\nlp_project_files\\\\'\n",
    "for file in os.listdir(r'C:\\Users\\user\\Desktop\\AI projects\\nlp_project_files'):\n",
    "    if  file != 'kone_classification.json':\n",
    "        df = pd.read_csv(f'{path}{file}')\n",
    "        all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "\n",
    "print(all_df.shape)\n",
    "all_df.to_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the jesonfile as dataframe\n",
    "json_file = \"C:\\\\Users\\\\user\\\\Desktop\\\\AI projects\\\\nlp_project_files\\\\kone_classification.json\"\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "    df_json=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The description of the dataset is: \\n\",df_json.info())\n",
    "print(\"The number of labels in the dataset is: \",df_json['label'].nunique())\n",
    "# count the rows for each language\n",
    "df_json.groupby('culture').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we obsarve that the dataset is imbalance and has a huge ratio of mineroty classes with such one or two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training source and drop the workflow\n",
    "df_json_training= df_json.loc[df_json['source']== 'TRAINING',:]\n",
    "df_json_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the French culture\n",
    "df_json_training_fr = df_json_training.loc[df_json_training['culture']=='fr-fr',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json_training_fr.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprosessing the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# some text cleaning functions\n",
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "\n",
    "def remove_special_char(text):\n",
    "    special_char = r'[^\\w\\s]|.:,*\"'\n",
    "    remove_special_char = re.sub(pattern=special_char, repl=\" \", string=text)\n",
    "    return remove_special_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\" Function to apply all in one\n",
    "    parameters: dataframe\n",
    "    return: dataframe  \"\"\"\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x: convert_to_lower(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_numbers(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_special_char(x))\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_data(df_json_training_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the code with one sentence from the dataset using Contextual Word Embeddings Augmenter (BERT)\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.1)\n",
    "text = df['text'].iloc[51]\n",
    "for i in range(3):  \n",
    "    augmented_text = aug.augment(text)\n",
    "    print(\"Augmented Text:\")\n",
    "    print(augmented_text)\n",
    "print(\"Original:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.1)\n",
    "\n",
    "def data_Aug(messege,aug_range=1):\n",
    "    \"\"\" Function for augmenting data using Contextual Word Embeddings Augmenter (BERT)\n",
    "    parameters: message: text from the dataset\n",
    "                aug_range: required sampels number\n",
    "                \n",
    "    return : one augmented message   \"\"\"\n",
    "\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        augmented_text = aug.augment(messege)\n",
    "        augmented_messages.append(str(augmented_text))\n",
    "        \n",
    "\n",
    "    return augmented_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for augmenting data using langauge translation\n",
    "## Could not found free service for langauge translation, Use paid service like Azure, Google translator etc\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "import random\n",
    "sr = random.SystemRandom()\n",
    "\n",
    "language = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "\n",
    "def data_augmentation(message, language, aug_range=1):\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "    for j in range(0,aug_range) :\n",
    "        new_message = \"\"\n",
    "        text = TextBlob(message)\n",
    "        try:\n",
    "            text = text.translate(to=sr.choice(language))   ## Converting to random langauge for meaningful variation\n",
    "            text = text.translate(to=\"en\")\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        augmented_messages.append(str(text))\n",
    "\n",
    "    return augmented_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = df.label.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "import numpy as np\n",
    "import math\n",
    "newdf = pd.DataFrame()   # the augmented dataframe\n",
    "for label, count in label_count.items() :\n",
    "    count_diff = max_label_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in df.loc[df[\"label\"] == label, \"text\" ]:\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['text'])\n",
    "            dummy1[\"label\"] = label\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_Aug(message,multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['text'])\n",
    "            dummy2[\"label\"] = label\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(df[df[\"label\"] == label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = newdf.label.value_counts().to_dict()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max label count to match other minority classes through data augmentation\n",
    "# import operator\n",
    "# max_label_count = max(label_count.items(), key=operator.itemgetter(1))[1]\n",
    "max_label_count= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "newdf2 = pd.DataFrame()   # the augmented dataframe\n",
    "for label, count in label_count.items() :\n",
    "    # print(label, count)\n",
    "    count_diff = max_label_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    # print(multiplication_count)\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in newdf.loc[newdf[\"label\"] == label, \"text\" ]:\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['text'])\n",
    "            dummy1[\"label\"] = label\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_Aug(message,multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['text'])\n",
    "            dummy2[\"label\"] = label\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf2 = newdf2.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf2 = newdf2.append(df[df[\"label\"] == label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we avoid to make complete balanced classes because our goal was to breake the huge gab between classes and to keep it close to reality as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print count of all new data points\n",
    "newdf2.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clean_df = newdf2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clean_df['label_id']=new_clean_df['label'].factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence embeding using sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed(data, model_embed):\n",
    "    sentences = data.values\n",
    "    embeddings = model_embed.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load the dataset\n",
    "def load_dataset(df):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdata = df\n",
    "\t# retrieve numpy array\n",
    "\tdata = df[['text', 'label']]\n",
    "\t# split into input and output elements\n",
    "\tX, y = data['text'], data['label']\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_clean_df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= embed(X,model_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "\n",
    "    LogisticRegression(random_state=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV,error_score='raise')\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='model_name', y='accuracy', \n",
    "            data=cv_df, \n",
    "            color='lightblue', \n",
    "            showmeans=True)\n",
    "plt.title(\"MEAN ACCURACY (cv = 5)n\", size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:\\\\Users\\\\user\\\\Desktop\\\\AI projects\\\\nlp_project_files\\\\LinearSVC_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df['y_test']= y_test\n",
    "predicted_df['y_pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('CLASSIFICATIION METRICSn')\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file):\n",
    "    \"\"\" Function to predict the classes for each entity using saved LinearSVC model\n",
    "    parameter: json_file\n",
    "    return: dataframe has the actual classes and the predicted classes \"\"\"\n",
    "\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        df_json=pd.DataFrame(data)\n",
    "    df_json_training= df_json.loc[df_json['source']== 'TRAINING',:]      # choose the training source and drop the workflow\n",
    "    df_json_training_fr = df_json_training.loc[df_json_training['culture']=='fr-fr',:]          # choose the French culture\n",
    "    df_cleaned = clean_data(df_json_training_fr)   # data preprocessing\n",
    "    X, y_test = load_dataset(df_cleaned)           # feature selection\n",
    "    new_X_test= embed(X,model_embed)                # implement the embeddings for the new test dataset\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))    # load the saved model\n",
    "    y_pred = loaded_model.predict(new_X_test)           # predict the classes\n",
    "    predicted_df = pd.DataFrame()                       # create a data frame that show the prediction result\n",
    "    predicted_df['y_test']= y_test\n",
    "    predicted_df['y_pred']=y_pred\n",
    "    \n",
    "    return predicted_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embedding using transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences we want sentence embeddings for\n",
    "sentences = list(df['text'].values)\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "319cceb38ce3abb27b8cff5600f422076befa028d2e26e408ae4af4c3a80a084"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
