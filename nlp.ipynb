{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file for all codes (labels) \n",
    "all_df = pd.DataFrame()\n",
    "path = 'C:\\\\Users\\\\user\\\\Desktop\\\\AI projects\\\\nlp_project_files\\\\'\n",
    "for file in os.listdir(r'C:\\Users\\user\\Desktop\\AI projects\\nlp_project_files'):\n",
    "    if  file != 'kone_classification.json':\n",
    "        df = pd.read_csv(f'{path}{file}')\n",
    "        all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "\n",
    "print(all_df.shape)\n",
    "all_df.to_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the jesonfile as dataframe\n",
    "json_file = \"C:\\\\Users\\\\user\\\\Desktop\\\\AI projects\\\\nlp_project_files\\\\kone_classification.json\"\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "    df_json=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The description of the dataset is: \\n\",df_json.describe())\n",
    "print(\"The number of labels in the dataset is: \",df_json['label'].nunique())\n",
    "# count the rows for each language\n",
    "df_json.groupby('culture').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training source and drop the workflow\n",
    "df_json_training= df_json.loc[df_json['source']== 'TRAINING',:]\n",
    "df_json_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the duplicated rows\n",
    "duplicateRows = df_json_training[df_json_training.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated rows\n",
    "df_json_training.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the French culture\n",
    "df_json_training_fr = df_json_training.loc[df_json_training['culture']=='fr-fr',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_json_training_fr['document_id'].nunique())\n",
    "print(df_json_training_fr['annotation_id'].nunique())\n",
    "# check how many unique labels are there\n",
    "print('the unique number of labels is: ',df_json_training_fr['label'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json_training_fr.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprosessing the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# some text cleaning functions\n",
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "\n",
    "def remove_special_char(text):\n",
    "    special_char = r'[^\\w\\s]|.:,*\"'\n",
    "    remove_special_char = re.sub(pattern=special_char, repl=\" \", string=text)\n",
    "    return remove_special_char\n",
    "df_json['text'] = df_json['text'].apply(lambda x: convert_to_lower(x))\n",
    "df_json['text'] = df_json['text'].apply(lambda x: remove_numbers(x))\n",
    "df_json['text'] = df_json['text'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "df_json['text'] = df_json['text'].apply(lambda x: remove_special_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_training_fr['text'] = df_json_training_fr['text'].apply(lambda x: convert_to_lower(x))\n",
    "df_json_training_fr['text'] = df_json_training_fr['text'].apply(lambda x: remove_numbers(x))\n",
    "df_json_training_fr['text'] = df_json_training_fr['text'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "df_json_training_fr['text'] = df_json_training_fr['text'].apply(lambda x: remove_special_char(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the code with one sentence from the dataset\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.1)\n",
    "text = df_json_training_fr['text'].iloc[199]\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.1)\n",
    "\n",
    "def data_Aug(messege,aug_range=1):\n",
    "    \"\"\" Function for augmenting data using Contextual Word Embeddings Augmenter (BERT)\n",
    "    parameters: message: text from the dataset\n",
    "                aug_range: required sampels number\n",
    "                \n",
    "    return : one augmented message   \"\"\"\n",
    "\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        augmented_text = aug.augment(messege)\n",
    "        augmented_messages.append(str(augmented_text))\n",
    "        \n",
    "\n",
    "    return augmented_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary for label counter\n",
    "label_count = df_json_training_fr.label.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max label count to match other minority classes through data augmentation\n",
    "import operator\n",
    "max_label_count = max(label_count.items(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "import numpy as np\n",
    "import math\n",
    "newdf = pd.DataFrame()   # the augmented dataframe\n",
    "for label, count in label_count.items() :\n",
    "    count_diff = max_label_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in df_json_training_fr.loc[df_json_training_fr[\"label\"] == label, \"text\" ]:\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['text'])\n",
    "            dummy1[\"label\"] = label\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_Aug(message,multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['text'])\n",
    "            dummy2[\"label\"] = label\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    # else :\n",
    "    #     newdf = newdf.append(df[df[\"label\"] == label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print count of all new data points\n",
    "newdf.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence embeding using sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['text'].values\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embedding using transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences we want sentence embeddings for\n",
    "sentences = list(df['text'].values)\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_training_fr['text'].iloc[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.fr.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df_json_training_fr['text'].iloc[18])\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent = []\n",
    "for word in doc:\n",
    "    if word.is_stop == False:\n",
    "      filtered_sent.append(word)\n",
    "print(filtered_sent)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "319cceb38ce3abb27b8cff5600f422076befa028d2e26e408ae4af4c3a80a084"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
