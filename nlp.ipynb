{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import functions\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the jesonfile as dataframe\n",
    "json_file = \"kone_classification.json\"\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "    df_json=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_json.info())\n",
    "print(\"The number of labels in the dataset is: \",df_json['label'].nunique())\n",
    "# count the rows for each language\n",
    "print(df_json.groupby('culture').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = df_json.label.value_counts().to_dict()\n",
    "classes_count\n",
    "count = 0\n",
    "for i in classes_count:\n",
    "    if classes_count[i] < 10:\n",
    "        count =count + 1\n",
    "print(f'This dataset has {count} minority classes with less than 10 sampels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we obsarve that the dataset is imbalance and has a huge ratio of minority classes with such one or two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training source and drop the workflow\n",
    "df_json_training= df_json.loc[df_json['source']== 'TRAINING',:]\n",
    "df_json_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the French culture\n",
    "df_json_training_fr = df_json_training.loc[df_json_training['culture']=='fr-fr',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum and minimum frequent for each label\n",
    "df_json_training_fr.groupby('label').count().sort_values(by=['text'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Destribution of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data, and see how many numbers of text are there per label\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "colors = ['grey','grey','grey','grey','grey','grey','grey','grey','grey',\n",
    "    'grey','darkblue','darkblue','darkblue']\n",
    "df_json_training_fr.groupby('label').text.count().sort_values().plot.barh(\n",
    "    ylim=0, color=colors, title= 'NUMBER OF Samples IN EACH label')\n",
    "plt.xlabel('Number of ocurrences', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_training_fr.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = df_json_training_fr.label.value_counts().to_dict()\n",
    "classes_count\n",
    "count = 0\n",
    "for i in classes_count:\n",
    "    if classes_count[i] < 10:\n",
    "        count =count + 1\n",
    "print(f'This dataset has {count} minority classes with less than 10 sampels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Preprosessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = functions.clean_data(df_json_training_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for augmenting data using langauge translation\n",
    "## Could not found free service for langauge translation, Use paid service like Azure, Google translator etc\n",
    "# sr = random.SystemRandom()\n",
    "\n",
    "language = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = df.label.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "newdf = pd.DataFrame()   # the augmented dataframe\n",
    "for label, count in label_count.items() :\n",
    "    count_diff = max_label_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in df.loc[df[\"label\"] == label, \"text\" ]:\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['text'])\n",
    "            dummy1[\"label\"] = label\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = functions.data_Aug(message,multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['text'])\n",
    "            dummy2[\"label\"] = label\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(df[df[\"label\"] == label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the counter for each label in the new dataframe \n",
    "label_count = newdf.label.value_counts().to_dict()\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the maximum number of new augmentated texts for the next dataframe\n",
    "max_label_count= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to augement the data using translation\n",
    "newdf2 = pd.DataFrame()   # the augmented dataframe\n",
    "for label, count in label_count.items() :\n",
    "    # print(label, count)\n",
    "    count_diff = max_label_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    # print(multiplication_count)\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in newdf.loc[newdf[\"label\"] == label, \"text\" ]:\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['text'])\n",
    "            dummy1[\"label\"] = label\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = functions.data_Aug(message,multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['text'])\n",
    "            dummy2[\"label\"] = label\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf2 = newdf2.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf2 = newdf2.append(df[df[\"label\"] == label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we avoid to make complete balanced classes because our goal was to breake the huge gab between classes and to keep it close to reality as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print count of all new data points\n",
    "newdf2.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicate after augmentation and drop them\n",
    "print(newdf2.duplicated().sum())\n",
    "new_clean_df = newdf2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_clean_df.to_csv('augmented_capital_dataset.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('augmented_dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_clean_df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset for training\n",
    "X, y_label,y = functions.load_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_frame = pd.DataFrame()\n",
    "labels_frame['label'] = y_label\n",
    "labels_frame['label_id']= y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_frame.groupby(['label','label_id']).count().sort_values(by=['label_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the sentence Embedding\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X= functions.embed(X,model_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV,error_score='raise')\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='model_name', y='accuracy', \n",
    "            data=cv_df, \n",
    "            color='lightblue', \n",
    "            showmeans=True)\n",
    "plt.title(\"MEAN ACCURACY (cv = 5)n\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test1, y_train, y_test1, y_train_label, y_test1_label = train_test_split(X, y, y_label,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test, y_val_label, y_test_label =  train_test_split(X_test1, y_test1, y_test1_label,\n",
    "                                                    test_size=0.50,\n",
    "                                                    random_state = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Models\\\\LinearSVC_main_model.sav'\n",
    "filename = 'Models\\\\LinearSVC_capital_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df['class']= y_val_label\n",
    "predicted_df['y_test']= y_val\n",
    "predicted_df['y_pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.iloc[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('CLASSIFICATIION METRICSn')\n",
    "print(metrics.classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Models\\\\LogisticRegrission_main_model.sav'\n",
    "filename = 'Models\\\\LogisticRegrission_sub_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df['class']= y_val_label\n",
    "predicted_df['y_test']= y_val\n",
    "predicted_df['y_pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('CLASSIFICATIION METRICSn')\n",
    "print(metrics.classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement RandomForestClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Models\\\\RandomForestClassifier_main_model.sav'\n",
    "filename = 'Models\\\\RandomForestClassifier_main_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df['class']= y_val_label\n",
    "predicted_df['y_test']= y_val\n",
    "predicted_df['y_pred']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('CLASSIFICATIION METRICSn')\n",
    "print(metrics.classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df[20:40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
